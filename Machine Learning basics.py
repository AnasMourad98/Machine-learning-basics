# -*- coding: utf-8 -*-
"""Untitled2 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1inPlAPHAvC_JQ-gbTtcufFes0kB-VrSU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler
import random
from sklearn.svm import SVC
import sklearn.metrics as sk
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

df= pd.read_csv('sgemm_product.csv')

df['Runtime']=df[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)
df

for index in range(10):
    df.iloc[:,index] = (df.iloc[:,index]-df.iloc[:,index].mean()) / df.iloc[:,index].std();
df.hist(figsize= (14,16));

median = df['Runtime'].median()

df.loc[df['Runtime'] <= median, 'Runtime'] = 0
df.loc[df['Runtime'] > median, 'Runtime'] = 1
m=df['Runtime'].value_counts()
print(median)
print(m)

X=df.iloc[:,0:14].values
Y=df.iloc[:,18].values

from sklearn.model_selection import train_test_split

# Use X_train, X_test, y_train, y_test for all of the following questions
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.dummy import DummyClassifier
from sklearn.metrics import recall_score, accuracy_score

dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)
y_dummy_predictions = dummy_majority.predict(X_test)
accuracy = accuracy_score(y_test, y_dummy_predictions)
recall = recall_score(y_test, y_dummy_predictions)               
print('dummy accurace :',accuracy)
print('dummy recall :',recall)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_dummy_predictions, target_names=['0', '1']))

from sklearn.metrics import recall_score, precision_score, accuracy_score
from sklearn.svm import SVC

svm = SVC().fit(X_train, y_train)
svm_predicted = svm.predict(X_test)

accuracy = accuracy_score(y_test, svm_predicted)
recall = recall_score(y_test, svm_predicted) 

precision = precision_score(y_test, svm_predicted) 
print('SVC accuracy :',accuracy)
print('SVC recall :',recall)
print('SVC precision :',precision)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

clf = SVC(kernel='rbf',C=10,gamma=0.01).fit(X_train, y_train)
svm1_predicted = clf.predict(X_test)

accuracy = accuracy_score(y_test, svm1_predicted)
recall = recall_score(y_test, svm1_predicted) 

precision = precision_score(y_test, svm1_predicted) 
print('SVC accuracy :',accuracy)
print('SVC recall :',recall)
print('SVC precision :',precision)

from sklearn.metrics import confusion_matrix

svm = SVC(C=1e9,gamma=1e-07).fit(X_train, y_train)
Decision_Function = svm.decision_function(X_test)
Decision_Function = Decision_Function >-220
confusion = confusion_matrix(y_test, Decision_Function)

accuracy = accuracy_score(y_test, Decision_Function)

print(accuracy)
print(confusion)

df_cm = pd.DataFrame(confusion, index = ['abovemediean', 'undermedian'],
                  columns = ['B', 'M'])

plt.figure(figsize = (5.5,4))
sns.heatmap(df_cm, annot=True)
plt.title('SVM dafault \nAccuracy:{0:.3f}'.format(accuracy))
plt.ylabel('True label')
plt.xlabel('Predicted label');

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score

log_reg = LogisticRegression(C=1).fit(X_train, y_train)
# Your code here

grid_values = {'C': [0.1, 1, 10, 100],
               'penalty': ['l1', 'l2']}
grid_clf_auc = GridSearchCV(log_reg, param_grid = grid_values, scoring = 'accuracy')
grid_clf_auc.fit(X_train, y_train)
y_decision_fn_scores_auc = grid_clf_auc.decision_function(X_test) 
scores = cross_val_score(grid_clf_auc, X, Y, cv=3, scoring = 'accuracy')
print('Cross-validation (accuracy)', scores)
print('Test set AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))
print('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)
print('Grid best score (AUC): ', grid_clf_auc.best_score_)
print("Best estimator:\n{}".format(grid_clf_auc.best_estimator_))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve, auc

log_reg = LogisticRegression().fit(X_train, y_train)
lr_probs = log_reg.predict_proba(X_test)
lr_probs = lr_probs[:, 1]
precision, recall , _= precision_recall_curve(y_test, lr_probs)

plt.figure()
plt.xlim([0.0, 1.01])
plt.ylim([0.0, 1.01])
plt.plot(precision, recall, label='Precision-Recall Curve')
plt.xlabel('Precision', fontsize=16)
plt.ylabel('Recall', fontsize=16)
plt.axes().set_aspect('equal')
plt.show()
    

    
fpr_lr , tpr_lr , _ =roc_curve(y_test, lr_probs)
roc_auc_lr = auc(fpr_lr, tpr_lr)



print(roc_auc_lr)


plt.figure()
plt.xlim([-0.01, 1.00])
plt.ylim([-0.01, 1.01])
plt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))
plt.xlabel('False Positive Rate', fontsize=16)
plt.ylabel('True Positive Rate', fontsize=16)
plt.title('ROC curve ', fontsize=16)
plt.legend(loc='lower right', fontsize=13)
plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')
plt.axes().set_aspect('equal')
plt.show()



from sklearn.metrics import confusion_matrix

y_majority_predicted = log_reg.predict(X_test)

confusion = confusion_matrix(y_test, y_majority_predicted)
print(confusion)

def train_using_gini(X_train, X_test, y_train):
    clf_gini = DecisionTreeClassifier(criterion = "gini",
            random_state = 100,max_depth=3, min_samples_leaf=5)
    clf_gini.fit(X_train, y_train)
    return clf_gini
      
def prediction(X_test, clf_object):
  
    # Predicton on test with giniIndex
    y_pred = clf_object.predict(X_test)
    print("Predicted values:")
    print(y_pred)
    return y_pred
      
# Function to calculate accuracy
def cal_accuracy(y_test, y_pred):
      
    print("Confusion Matrix:\n ",
        confusion_matrix(y_test, y_pred))
      
    print ("Accuracy : ",
    accuracy_score(y_test,y_pred)*100)
      
    print("Report : ",
    classification_report(y_test, y_pred))
clf_gini = train_using_gini(X_train, X_test, y_train)

# Operational Phase
print("Results Using Gini Index:")
     
# Prediction using gini
y_pred_gini = prediction(X_test, clf_gini)
cal_accuracy(y_test, y_pred_gini)

print(clf_gini.feature_importances_)
import numpy as np
n_features = X.shape[1]
plt.barh(range(n_features), clf_gini.feature_importances_, align='center')
plt.yticks(np.arange(n_features))
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.show()

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg.fit(X, Y)


cross_val_score(tree_reg, X, Y, cv=10)

tree_reg = DecisionTreeRegressor(max_depth=3, random_state=42)
tree_reg.fit(X, Y)


cross_val_score(tree_reg, X, Y, cv=10)

from sklearn.tree import export_graphviz
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, Y)
export_graphviz(
        tree_clf,
        feature_names=df.columns[0:14],
        rounded=True,
        filled=True
    )

from sklearn.feature_selection import VarianceThreshold
sel_var = VarianceThreshold(threshold=(.8 * (1 - .8)))
sel_var.fit(X_train, y_train)
# transform training set
X_train_selected = sel_var.transform(X_train)

import matplotlib.pyplot as plt
mask = sel_var.get_support()
print(mask)
# visualize the mask. black is True, white is False
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("Sample index")
plt.yticks(())

from sklearn.feature_selection import SelectPercentile

# use f_classif (the default) and SelectPercentile to select 50% of features
select = SelectPercentile(percentile=50)
select.fit(X_train, y_train)
# transform training set
X_train_selected = select.transform(X_train)

print("X_train.shape: {}".format(X_train.shape))
print("X_train_selected.shape: {}".format(X_train_selected.shape))

import matplotlib.pyplot as plt
mask = select.get_support()
print(mask)
# visualize the mask. black is True, white is False
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("Sample index")
plt.yticks(())

from sklearn.linear_model import LogisticRegression

# transform test data
X_test_selected = select.transform(X_test)
#lr = SGDClassifier(loss='log',penalty ='none',max_iter = 5000,shuffle=False,eta0=0.1,learning_rate='constant')

lr = SVC();
lr.fit(X_train, y_train)
print("Score with all features: {:.3f}".format(lr.score(X_test, y_test)))

lr.fit(X_train_selected, y_train)
print("Score with only selected features: {:.3f}".format(
    lr.score(X_test_selected, y_test)))

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
select = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=42),
    threshold="median")

select.fit(X_train, y_train)
X_train_l1 = select.transform(X_train)
print("X_train.shape: {}".format(X_train.shape))
print("X_train_l1.shape: {}".format(X_train_l1.shape))

mask = select.get_support()
# visualize the mask. black is True, white is False
plt.matshow(mask.reshape(1, -1), cmap='gray_r')
plt.xlabel("Sample index")
plt.yticks(())

X_test_l1 = select.transform(X_test)
score = lr.fit(X_train_l1, y_train).score(X_test_l1, y_test)
print("Test score: {:.3f}".format(score))



X_train_rfe = select.transform(X_train)
X_test_rfe = select.transform(X_test)
score = lr.fit(X_train_rfe, y_train).score(X_test_rfe, y_test)
print("Test score: {:.3f}".format(score))

from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression



from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures

model = make_pipeline(
                      PolynomialFeatures(degree=2),
                      LinearRegression(),)

model.fit(X, Y) 
print(model.predict(X))

from sklearn.ensemble import RandomForestClassifier


forest = RandomForestClassifier(n_estimators=2,random_state=0)
forest.fit(X_train, y_train)
y_pred_test = forest.predict(X_test)

accuracy_score(y_test, y_pred_test)

from sklearn.ensemble import RandomForestClassifier


forest1 = RandomForestClassifier(n_estimators=100,random_state=0)
forest1.fit(X_train, y_train)
y_pred1_test = forest1.predict(X_test)

accuracy_score(y_test, y_pred1_test)

